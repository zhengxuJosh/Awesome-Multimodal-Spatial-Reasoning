# General_MLLM

| Title | Authors | Venue/Date | Paper Link | Code | Entire/Partial | Modal | Remarks |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ***Benchmark and Dataset*** |  |  |  |  |  |  |  |
| Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation | Liao *et al.* | Arxiv 2025 (Oct) | [paper](https://arxiv.org/pdf/2510.08673) | https://github.com/KangLiao929/Puffin | Entire | Image-Text-Camera |  |
| Video-R1: Reinforcing Video Reasoning in MLLMs | Feng *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.21776) | https://github.com/tulerfeng/Video-R1 | Partial | Image-Text |  |
| Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding | Imran Kabir *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.12663) | https://github.com/Imran2205/LogicRAG | Entire | Vedio-Text |  |
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | [Peiran Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+P) *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.12542) | / | Entire | Vedio-Text |  |
| How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game | [Ziyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Z) *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.10042) | https://github.com/THUNLP-MT/EscapeCraft | Entire | Image-Text |  |
| ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models | [Jonathan Roberts](https://arxiv.org/search/cs?searchtype=author&query=Roberts,+J) *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/abs/2502.09696) | [Github](https://zerobench.github.io/) | Entire | Image-Text |  |
| LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations | [Mingjie Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+M) *et al.* | WACV 2025 | [paper](https://arxiv.org/abs/2412.06322) | https://github.com/Endlinc/LLaVA-SpaceSGG | Entire | Graph-Desc/QA/Conv |  |
| LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding | [Hongyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+H) *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.08282) | https://github.com/appletea233/LLaVA-ST | Entire | Vedio-Text(QA) |  |
| Thinking in space: How multimodal large language models see, remember, and recall spaces | Yang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.14171) | [code](https://github.com/vision-x-nyu/thinking-in-space) | Entire | Vedio-Text(QA) |  |
| Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models | [Xingrui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X) *et al.* | CVPR 2025 | [paper](https://arxiv.org/abs/2502.08636) | https://github.com/XingruiWang/Spatial457 | Entire | Image-Text |  |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | Liao *et al.* | Arxiv 2025 (Apl) | [paper](https://arxiv.org/pdf/2504.00883) | https://github.com/zhijie-group/R1-Zero-VSI | Entire | Vedio-Text(QA) |  |
| Imagine while Reasoning in Space: Multimodal Visualization-of-Thought | [Chengzu Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+C) *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.07542) | / | Entire | Image-Text |  |
| MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models | [Huanqia Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai,+H) *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/abs/2502.00698) | [Github](https://acechq.github.io/MMIQ-benchmark/) | Entire | Image-Text | Doubtful |
| CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs | Siyu Wang *et al.* | AAAI 2025 | [paper](https://ojs.aaai.org/index.php/AAAI/article/view/32849) | [Github](https://openiwin.github.io/CAD-GPT/) | Entire | CAD-Text | Doubtful |
| GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs | [Navid Rajabi](https://arxiv.org/search/cs?searchtype=author&query=Rajabi,+N) *et al.* | NIPS 2024 Workshop | [paper](https://arxiv.org/abs/2406.13246) | / | Entire | Image-Text(QA) |  |
| DriveLM: Driving with Graph Visual Question Answering | [Chonghao Sima](https://arxiv.org/search/cs?searchtype=author&query=Sima,+C) *et al.* | ECCV 2024 | [paper](https://arxiv.org/abs/2312.14150) | https://github.com/OpenDriveLab/DriveLM | Entire | Image/Graph-Text(QA) |  |
| Spatial Task-Explicity Matters in Prompting Large Multimodal Models for Spatial Planning | Ivan Majic *et al.* | GeoAI 2024 | [paper](https://dl.acm.org/doi/abs/10.1145/3687123.3698293) | https://github.com/ivan-majic/llm_modality_reasoning | Entire | Image-Text |  |
| ABenchmark Dataset for Evaluating Spatial Perception in Multimodal Large Models | Li Xuan *et al.* | [IOTMMIM 24](https://dl.acm.org/doi/proceedings/10.1145/3698385) | [paper](https://dl.acm.org/doi/abs/10.1145/3698385.3699875) | / | Entire | Image-Text |  |
| PUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns | [Yew Ken Chia](https://arxiv.org/search/cs?searchtype=author&query=Chia,+Y+K) *et al.* | ACL 2024 | [paper](https://arxiv.org/abs/2403.13315) | https://github.com/declare-lab/LLM-PuzzleTest | Entire | Image-Text | Doubtful |
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | [An-Chieh Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng,+A) *et al.* | NIPS 2024 | [paper](https://arxiv.org/abs/2406.01584) | [Github](https://www.anjiecheng.me/SpatialRGPT) | Entire | Image-Text(QA) |  |
| SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models | [Arijit Ray](https://arxiv.org/search/cs?searchtype=author&query=Ray,+A) *et al.* | arxiv 2024(Dec) | [paper](https://arxiv.org/abs/2412.07755) | [Huggingface](https://huggingface.co/datasets/array/SAT) | Entire | Image-Text(QA) |  |
| BLINK : Multimodal Large Language Models
Can See but Not Perceive | Xingyu Fu *et al.* | arxiv 2024(Apr) | [paper](https://arxiv.org/pdf/2404.12390) | [Github](https://zeyofu.github.io/blink/) | Entire | Image-Text(QA) |  |
| Does Spatial Cognition Emerge in Frontier Models? | Ramakrishnan *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.06468) | / | Entire | Image-Text |  |
| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models | Wang *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2406.14852) | [code](https://github.com/jiayuww/SpatialEval) | Entire | Image-Text |  |
| CityGPT: Empowering Urban Spatial Cognition of Large Language Models | Feng *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2406.13948) | [code](https://github.com/tsinghua-fib-lab/CityGPT) | Partial | Map/Image/Geo-Text |  |
| DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving | Guo *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13112) | [code](https://github.com/XiandaGuo/Drive-MLLM) | Entire | Image-Text(QA) |  |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | [Boyuan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+B) *et al.* | arxiv 2024(Jan) | [paper](https://arxiv.org/abs/2401.12168) | [Github](https://spatial-vlm.github.io/) | Entire | Image-Text |  |
| EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models | [Mengfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du,+M) *et al.* | ACL 2024 | [paper](https://arxiv.org/abs/2406.05756) | https://github.com/mengfeidu/EmbSpatial-Bench | Entire | Image-Text(QA) |  |
| AirVista: Empowering UAVs with 3D Spatial Reasoning Abilities Through a Multimodal Large Language Model Agent | Fei Lin *et al.* | ITSC 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10919532) | / | Entire | Image-Text |  |
| What's "up" with vision-language models? Investigating their struggle with spatial reasoning | [Amita Kamath](https://arxiv.org/search/cs?searchtype=author&query=Kamath,+A) *et al.* | EMNLP 2023 | [paper](https://arxiv.org/abs/2310.19785) | https://github.com/amitakamath/whatsup_vlms | Entire | Image-Text |  |
| Visual Spatial Reasoning | Liu *et al.* | TACL Volume 11 2023 | [paper](https://arxiv.org/pdf/2205.00363) | [code](https://github.com/cambridgeltl/visual-spatial-reasoning) | Entire | Image-Text |  |
| VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena | Letitia Parcalabescu *et al.* | ACL 2022 | [paper](https://arxiv.org/abs/2112.07566) | https://github.com/Heidelberg-NLP/VALSE | Partial | Image-Text |  |
| Things not written in text: Exploring spatial commonsense from visual signals | Xiao Liu *et al.* | ACL 2022 | [paper](https://arxiv.org/abs/2203.08075) | https://github.com/xxxiaol/spatial-commonsense | Entire | Image-Text |  |
| SpartQA: : A Textual Question Answering Benchmark for Spatial Reasoning | [Roshanak Mirzaee](https://arxiv.org/search/cs?searchtype=author&query=Mirzaee,+R) *et al.* | NAACL 2021 | [paper](https://arxiv.org/abs/2104.05832) | https://github.com/HLR/SpartQA_generation | Entire | Text |  |
| 2.5D Visual Relationship Detection | [Yu-Chuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su,+Y) *et al.* | arxiv 2021(Apr) | [paper](https://arxiv.org/abs/2104.12727) | https://github.com/google-research-datasets/2.5vrd | Entire | Image-Text |  |
| PIP: Physical Interaction Prediction via Mental Simulation with Span Selection | [Jiafei Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan,+J) *et al.* | arxiv 2021(Sep) | [paper](https://arxiv.org/abs/2109.04683) | / | Entire | Vedio-Text(Classify) |  |
| TVQA+: Spatio-temporal grounding for video question answering | [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei,+J) *et al.* | ACL 2020 | [paper](https://arxiv.org/abs/1904.11574) | https://github.com/jayleicn/TVQAplus | Entire | Vedio-Text |  |
| Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D | [Ankit Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal,+A) *et al.* | NIPS 2020 | [paper](https://arxiv.org/abs/2012.01634) | https://github.com/princeton-vl/Rel3D | Entire | Image-Text |  |
| SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition | [Kaiyu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+K) *et al.* | ICCV 2019 | [paper](https://arxiv.org/abs/1908.02660) | https://github.com/princeton-vl/SpatialSense | Entire | Image-Text |  |
| Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates | Guillem Collell *et al.* | AAAI 2018 | [paper](https://arxiv.org/abs/1711.06821) | https://github.com/gcollell/spatial-commonsense | Entire | Image-Text |  |
| Visual Genome: Connecting language and vision using crowdsourced dense image annotations | Ranjay Krishna *et al.* | IJCV 2017 | [paper](https://arxiv.org/abs/1602.07332) | [Code](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) | Entire | Image-Text
Sence Graph |  |
| MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs | *Jihyung Kil et al.* | NIPS 2017 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/32923dff09f75cf1974c145764a523e2-Abstract-Datasets_and_Benchmarks_Track.html) | [Github](https://compbench.github.io/) | Partial | Image-Text(QA) |  |
| Stating the Obvious: Extracting Visual Common Sense Knowledge | Mark Yatskar *et al.* | NAACL 2016 | [paper](https://aclanthology.org/N16-1023/) | / (extract from COCO) | Entire | Text |  |