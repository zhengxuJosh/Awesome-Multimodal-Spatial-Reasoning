# General_MLLM

| Title | Authors | Venue/Date | Paper Link | Code | Entire/Partial | Modal |
| --- | --- | --- | --- | --- | --- | --- |
| ***Benchmark and Dataset*** |  |  |  |  |  |  |
| Does Spatial Cognition Emerge in Frontier Models? | Ramakrishnan *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.06468) | / | Entire | Image-Text |
| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models | Wang *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2406.14852) | [code](https://github.com/jiayuww/SpatialEval) | Entire | Image-Text |
| Visual Spatial Reasoning | Liu *et al.* | TACL Volume 11 2023 | [paper](https://arxiv.org/pdf/2205.00363) | [code](https://github.com/cambridgeltl/visual-spatial-reasoning) | Entire | Image-Text |
| Thinking in space: How multimodal large language models see, remember, and recall spaces | Yang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.14171) | [code](https://github.com/vision-x-nyu/thinking-in-space) |  |  |
| DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving | Guo *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13112) | [code](https://github.com/XiandaGuo/Drive-MLLM) |  |  |
| **VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena** | Letitia Parcalabescu *et al.* | ACL 2022 | [https://arxiv.org/abs/2112.07566](https://arxiv.org/abs/2112.07566) | https://github.com/Heidelberg-NLP/VALSE | Partial | Image-Text |
| Things not written in text: Exploring
spatial commonsense from visual signals | Xiao Liu *et al.* | ACL 2022 | [https://arxiv.org/abs/2203.08075](https://arxiv.org/abs/2203.08075) | https://github.com/xxxiaol/spatial-commonsense | Entire | Image-Text |
| **SpartQA: : A Textual Question Answering Benchmark for Spatial Reasoning** | [Roshanak Mirzaee](https://arxiv.org/search/cs?searchtype=author&query=Mirzaee,+R) *et al.* | NAACL 2021 | [https://arxiv.org/abs/2104.05832](https://arxiv.org/abs/2104.05832) | https://github.com/HLR/SpartQA_generation | Entire | Text |
| TVQA+: Spatio-temporal grounding
for video question answering | [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei,+J) *et al.* | ACL 2020 | [https://arxiv.org/abs/1904.11574](https://arxiv.org/abs/1904.11574) | https://github.com/jayleicn/TVQAplus | Entire | Vedio-Text |
| Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates | Guillem Collell *et al.* | AAAI 2018 | [https://arxiv.org/abs/1711.06821](https://arxiv.org/abs/1711.06821) | https://github.com/gcollell/spatial-commonsense | Entire | Image-Text |
| Visual Genome: Connecting language and vision using crowdsourced dense image annotations | Ranjay Krishna *et al.* | IJCV 2017 | [https://arxiv.org/abs/1602.07332](https://arxiv.org/abs/1602.07332) | [https://homes.cs.washington.edu/~ranjay/visualgenome/index.html](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) | Entire | Image-Text
Sence Graph |
| Stating the Obvious: Extracting Visual Common Sense Knowledge | Mark Yatskar *et al.* | NAACL 2016 | [https://aclanthology.org/N16-1023/](https://aclanthology.org/N16-1023/) | / (extract from COCO) | Entire | Text |
| ***Prompt Engineering*** |  |  |  |  |  |  |
| ***Post-training OR Fine-tuning*** |  |  |  |  |  |  |
| CityGPT: Empowering Urban Spatial Cognition of Large Language Models | Feng *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2406.13948) | [code](https://github.com/tsinghua-fib-lab/CityGPT) |  |  |
| Video-R1: Reinforcing Video Reasoning in MLLMs | Feng *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.21776) | [code](https://github.com/tulerfeng/Video-R1) |  |  |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | Liao *et al.* | Arxiv 2025 (Apl) | [paper](https://arxiv.org/pdf/2504.00883) | / |  |  |
| ***Explainability*** |  |  |  |  |  |  |
| Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models | Rajabi *et al.* | ICLR 2024 Workshop | [paper](https://arxiv.org/pdf/2308.09778) | / |  |  |
| ***Architecture*** |  |  |  |  |  |  |
| MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving | Zhang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2504.00379) | / |  |  |
|  |  |  |  |  |  |  |