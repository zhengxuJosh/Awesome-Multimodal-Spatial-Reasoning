### Benchmark and Dataset
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| Does Spatial Cognition Emerge in Frontier Models? | Ramakrishnan *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.06468) | / |
| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models | Wang *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2406.14852) | [code](https://github.com/jiayuww/SpatialEval) |
| Visual Spatial Reasoning | Liu *et al.* | TACL Volume 11 2023 | [paper](https://arxiv.org/pdf/2205.00363) | [code](https://github.com/cambridgeltl/visual-spatial-reasoning) |
| Thinking in space: How multimodal large language models see, remember, and recall spaces | Yang *et al.*   | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.14171)  | [code](https://github.com/vision-x-nyu/thinking-in-space) |  
| From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D | Zhang *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2503.22976) | [code](https://github.com/fudan-zvg/spar) |
| Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models | Stogiannidis  *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19707) | [code](https://github.com/stogiannidis/srbench) | 
| What's "up" with vision-language models? Investigating their struggle with spatial reasoning | Kamath *et al.* | EMNLP 2023 | [paper](https://arxiv.org/pdf/2310.19785) | [code](https://github.com/amitakamath/whatsup_vlms) | 
| TopViewRS: Vision-Language Models as Top-View Spatial Reasoners | Li *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2406.02537) | [code](https://github.com/cambridgeltl/topviewrs) | 
| An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models | Shiri *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2411.06048) | [code](https://github.com/FatemehShiri/Spatial-MM) | 
| Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space | Zhang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.11094) | [code](https://github.com/WeichenZh/Open3DVQA) |
| Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities | Zhang *et al.* | ICLR 2025 (Oral) | [paper](https://arxiv.org/pdf/2410.17385) | [code](https://github.com/sled-group/COMFORT) |  
| ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models | Li *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.21500) | [code](https://github.com/ZJU-REAL/ViewSpatial-Bench) |  
| OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models | Jia *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.03135) | [code](https://github.com/qizekun/OmniSpatial) | 
| InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models | Deng *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.18385) | / | 
| BLINK: Multimodal Large Language Models Can See but Not Perceive | Fu *et al.* | Arxiv 2024(Apr) | [paper](https://arxiv.org/pdf/2404.12390) | [code](https://zeyofu.github.io/blink/) |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | Chen *et al.* | arxiv 2024(Jan) | [paper](https://arxiv.org/pdf/2401.12168) | [code](https://spatial-vlm.github.io/) |
| Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models | Wang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2502.08636) | [code](https://github.com/XingruiWang/Spatial457) |


### Test-time Scaling
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models | Wu *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2404.03622) | [code](https://github.com/microsoft/visualization-of-thought) | 
| I Know About "Up"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction | Meng *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2407.14133) | [code](https://github.com/zhouhao028/Iknow_up) | 
| Do Large Language Models have Spatial Cognitive Abilities? | Wu *et al.* | ACM Transactions on Intelligent Systems and Technology | [paper](https://dl.acm.org/doi/pdf/10.1145/3716855) | [code](https://github.com/LLING000/SCABenchmark) | 
| Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models | Zhou *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.13872) | / | 
| SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding | Wu *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2505.17012) | [code](https://github.com/haoningwu3639/SpatialScore/) | 
| Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models | Zhu *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2506.04220) | [code](https://github.com/neu-vi/struct2d) |
| Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation | Lee *et al.* | ICCV 2025 | [paper](https://arxiv.org/pdf/2504.17207) | [code](https://github.com/KAIST-Visual-AI-Group/APC-VLM) |
| Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames | Ravi *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2505.24257) | / |
| SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors | Ma *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2403.13438) | [code](https://github.com/dannymcy/zeroshot_task_hallucination_code) |
| Spatial Understanding from Videos: Structured Prompts Meet Simulation Data | Zhang *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.03642) | [code](https://github.com/Hyu-Zhang/SpatialMind) |
| Visual Agentic AI for Spatial Reasoning with a Dynamic API | Marsili *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2502.06787) | [code](https://github.com/damianomarsili/VADAR) |
| Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models | Liao *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2409.09788) | [code](https://github.com/andrewliao11/Q-Spatial-Bench-code) |
| VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search
 | Wang *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.09130) | [code](https://github.com/ekonwang/VisuoThink) |

### Post-training OR Fine-tuning
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| CityGPT: Empowering Urban Spatial Cognition of Large Language Models | Feng *et al.* | KDD 2025 | [paper](https://arxiv.org/pdf/2406.13948) | [code](https://github.com/tsinghua-fib-lab/CityGPT) |
| Video-R1: Reinforcing Video Reasoning in MLLMs | Feng *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.21776) | [code](https://github.com/tulerfeng/Video-R1) |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | Liao *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.00883) | / |
| Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning | Ouyang *et al.* | Arxiv 2024 (Apr) | [pdf](https://arxiv.org/pdf/2504.01805) | [code](https://github.com/OuyangKun10/Spatial-R1) |
| MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse | Pan *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.18470) | [code](https://github.com/PzySeere/MetaSpatial) | 
| Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning | Tang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2410.16162) | / | 
| ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models | Ko *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19355) | [code](https://github.com/mlvlab/ST-VLM) | 
| Imagine while Reasoning in Space: Multimodal Visualization-of-Thought |  Li *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.07542) | / | 
| SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models |  Ray *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2412.07755) | [code](https://arijitray.com/SAT/) | 
| Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models |  Xu *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.17015) | [code](https://github.com/facebookresearch/Multi-SpatialMLLM) | 
| SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models | Guo *et al.*   | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13112)  | [code](https://github.com/XiandaGuo/Drive-MLLM) |
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | Wu *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.12542) | / |
| SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models | Ma *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2505.00788) | / |
| SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning | Ma *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2504.20024) | [code](https://github.com/johnson111788/SpatialReasoner) |
| Enhancing Spatial Reasoning through Visual and Textual Thinking | Liang *et al.* | Arxiv 2025 (Jul) | [paper](https://arxiv.org/pdf/2507.20529) | / |
| SpaceR: Reinforcing MLLMs in Video Spatial Reasoning | Ouyang, *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2504.01805) | [code](https://github.com/OuyangKun10/SpaceR) |
| Spatial Mental Modeling from Limited Views | Yin *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.21458) | [code](https://github.com/mll-lab-nu/MindCube) |
| SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data | Ogezi *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.20648) | / |
| M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning | Wang  *et al.* | Arxiv 2025 (Jul) | [paper](https://arxiv.org/pdf/2507.08306) | [code](https://github.com/inclusionAI/M2-Reasoning) |
| LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding | Li *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.08282) | [code](https://github.com/appletea233/LLaVA-ST) |
| MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence | Liu *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2505.10604) | [code](https://github.com/khazic/Mirage) |
| Can Multimodal Large Language Models Understand Spatial Relations? | Liu *et al.* | ACL 2025 | [paper](https://arxiv.org/pdf/2505.19015) | [code](https://github.com/ziyan-xiaoyu/SpatialMQA) |
| SITE: towards Spatial Intelligence Thorough Evaluation | Wang *et al.* | ICCV 2025 | [paper](https://arxiv.org/pdf/2505.05456) | [code](https://github.com/wenqi-wang20/SITE-Bench) |
| MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence | Yang *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.23764) | [code](https://github.com/InternRobotics/MMSI-Bench) |
| NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving | Tian *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.03164) | [code](https://taco-group.github.io/NuScenes-SpatialQA/) |

### Explainability
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models | Rajabi *et al.* | ICLR 2024 Workshop | [paper](https://arxiv.org/pdf/2308.09778) | / |
| Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models | Qi *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.17349) | / |
| Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas | Chen *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.01773) | [code](https://github.com/shiqichen17/AdaptVis) | 
| Can Transformers Capture Spatial Relations between Objects? | Wen *et al.* | ICLR 2024 | [paper](https://arxiv.org/pdf/2403.00729) | [code](https://github.com/AlvinWen428/spatial-relation-benchmark) |
| Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics | Xu *et al.* | ACL 2025 | [paper](https://arxiv.org/abs/2502.11859v2) | / |
| Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture | Zhang *et al.* | Arxiv 2025 (Sep) | [paper](https://arxiv.org/abs/2509.02359) | [code](https://github.com/WanyueZhang-ai/spatial-understanding) |

### Architecture
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Cheng *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.01584) | [code](https://www.anjiecheng.me/SpatialRGPT) |
| MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving | Zhang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2504.00379) | / |
| Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs | Ranasinghe *et al.* | CVPR 2024 | [paper](https://arxiv.org/pdf/2404.07449) | / |
| Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence | Wu *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.23747) | [code](https://github.com/diankun-wu/Spatial-MLLM) |
| Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model | He *et al.* | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.08199) | / |
| SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning | Liu *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.12448) | [code](https://github.com/yliu-cs/SSR) |
| SpatialBot: Precise Spatial Understanding with Vision Language Models | Cai *et al.* | ICRA 2025 | [paper](https://arxiv.org/pdf/2406.13642) | [code](https://github.com/BAAI-DCAI/SpatialBot) |
| Spatio-Temporal LLM: Reasoning about Environments and Actions | Zheng *et al.* | Arxiv 2025 (Jul) | [paper](https://arxiv.org/pdf/2507.05258) | [code](https://github.com/zoezheng126/Spatio-Temporal-LLM) |
| MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs | Daxberger *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.13111) | / |
| Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs | Tong *et al.* | NeurIPS 2024 (Oral) | [paper](https://arxiv.org/pdf/2406.16860) | [code](https://github.com/cambrian-mllm/cambrian) |

<!-- ### More Benchmark and Dataset
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
|---------------------------------------------------------------------------------------------------|---------------|------------------|
| LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? | Tang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19990) | [code](https://github.com/Tangkexian/LEGO-Puzzles) |----------------------------------------------|----------------------------------------------|
| Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding | Kabir *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.12663) | [code](https://github.com/Imran2205/LogicRAG) |
| How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game | Wang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.10042) | [code](https://github.com/THUNLP-MT/EscapeCraft) |
| ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models | Roberts *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.09696) | [code](https://zerobench.github.io/) |
| LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations | Xu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2412.06322) | [code](https://github.com/Endlinc/LLaVA-SpaceSGG) |
| PUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns | Chia *et al.* | ACL 2024 | [paper](https://arxiv.org/pdf/2403.13315) | [code](https://github.com/declare-lab/LLM-PuzzleTest) |
| EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models | Du *et al.* | ACL 2024 | [paper](https://arxiv.org/pdf/2406.05756) | [code](https://github.com/mengfeidu/EmbSpatial-Bench) |
| AirVista: Empowering UAVs with 3D Spatial Reasoning Abilities Through a Multimodal Large Language Model Agent | Lin *et al.* | ITSC 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10919532) | / |
| VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena | Parcalabescu *et al.* | ACL 2022 | [paper](https://arxiv.org/pdf/2112.07566) | [code](https://github.com/Heidelberg-NLP/VALSE) |
| Things not written in text: Exploring spatial commonsense from visual signals | Liu *et al.* | ACL 2022 | [paper](https://arxiv.org/pdf/2203.08075) | [code](https://github.com/xxxiaol/spatial-commonsense) |
| 2.5D Visual Relationship Detection | Su *et al.* | Arxiv 2021 (Apr) | [paper](https://arxiv.org/pdf/2104.12727) | [code](https://github.com/google-research-datasets/2.5vrd) |
| PIP: Physical Interaction Prediction via Mental Simulation with Span Selection | Duan *et al.* | Arxiv 2021 (Sep) | [paper](https://arxiv.org/pdf/2109.04683) | / |
| TVQA+: Spatio-temporal grounding for video question answering | Lei *et al.* | ACL 2020 | [paper](https://arxiv.org/pdf/1904.11574) | [code](https://github.com/jayleicn/TVQAplus) |
| Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D | Goyal *et al.* | NIPS 2020 | [paper](https://arxiv.org/pdf/2012.01634) | [code](https://github.com/princeton-vl/Rel3D) |
| Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates | Collell *et al.* | AAAI 2018 | [paper](https://arxiv.org/pdf/1711.06821) | [code](https://github.com/gcollell/spatial-commonsense) |
| Visual Genome: Connecting language and vision using crowdsourced dense image annotations | Krishna *et al.* | IJCV 2017 | [paper](https://arxiv.org/pdf/1602.07332) | [code](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) |
| Stating the Obvious: Extracting Visual Common Sense Knowledge | Yatskar *et al.* | NAACL 2016 | [paper](https://aclanthology.org/N16-1023/) | / | -->