| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ***Benchmark and Dataset*** |||||
| Does Spatial Cognition Emerge in Frontier Models? | Ramakrishnan *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.06468) | / |
| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models | Wang *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2406.14852) | [code](https://github.com/jiayuww/SpatialEval) |
| Visual Spatial Reasoning | Liu *et al.* | TACL Volume 11 2023 | [paper](https://arxiv.org/pdf/2205.00363) | [code](https://github.com/cambridgeltl/visual-spatial-reasoning) |
| Thinking in space: How multimodal large language models see, remember, and recall spaces | Yang *et al.*   | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.14171)  | [code](https://github.com/vision-x-nyu/thinking-in-space) |  
| From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D | Zhang *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2503.22976) | [code](https://github.com/fudan-zvg/spar) |
| LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? | Tang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19990) | [code](https://github.com/Tangkexian/LEGO-Puzzles) |
| Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models | Stogiannidis  *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19707) | [code](https://github.com/stogiannidis/srbench) | 
| What's "up" with vision-language models? Investigating their struggle with spatial reasoning | Kamath *et al.* | EMNLP 2023 | [paper](https://arxiv.org/pdf/2310.19785) | [code](https://github.com/amitakamath/whatsup_vlms) | 
| TopViewRS: Vision-Language Models as Top-View Spatial Reasoners | Li *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2406.02537) | [code](https://github.com/cambridgeltl/topviewrs) | 
| Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities | Zhang *et al.* | ICLR 2025 (Oral) | [paper](https://arxiv.org/pdf/2410.17385) | [code](https://huggingface.co/datasets/sled-umich/COMFORT) | 
| An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models | Shiri *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2411.06048) | [code](https://github.com/FatemehShiri/Spatial-MM) | 
| Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space | Zhang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.11094) | [code](https://github.com/WeichenZh/Open3DVQA) |
| Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities | Zhang *et al.* | ICLR 2025 (Oral) | [paper](https://arxiv.org/pdf/2410.17385) | [code](https://github.com/sled-group/COMFORT) |  
| ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models | Li *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.21500) | [code](https://github.com/ZJU-REAL/ViewSpatial-Bench) |  
| OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models | Jia *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.03135) | [code](https://github.com/qizekun/OmniSpatial) | 
| InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models | Deng *et al.* | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.18385) | / | 
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Cheng *et al.* | NIPS 2024 | [paper](https://arxiv.org/abs/2406.01584) | [code](https://www.anjiecheng.me/SpatialRGPT) |
| BLINK: Multimodal Large Language Models Can See but Not Perceive | Fu *et al.* | Arxiv 2024(Apr) | [paper](https://arxiv.org/pdf/2404.12390) | [code](https://zeyofu.github.io/blink/) |
| DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving | Guo *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13112) | [code](https://github.com/XiandaGuo/Drive-MLLM) |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | Chen *et al.* | arxiv 2024(Jan) | [paper](https://arxiv.org/abs/2401.12168) | [code](https://spatial-vlm.github.io/) |
| Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models | Wang *et al.* | CVPR 2025 | [paper](https://arxiv.org/abs/2502.08636) | [code](https://github.com/XingruiWang/Spatial457) |
| SpartQA: A Textual Question Answering Benchmark for Spatial Reasoning | Mirzaee *et al.* | NAACL 2021 | [paper](https://arxiv.org/abs/2104.05832) | [code](https://github.com/HLR/SpartQA_generation) |
| SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition | Yang *et al.* | ICCV 2019 | [paper](https://arxiv.org/abs/1908.02660) | [code](https://github.com/princeton-vl/SpatialSense) |
| ***Test-time Scaling*** |||||
| Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models | Wu *et al.* | NeurIPS 2024 | [paper](https://arxiv.org/pdf/2404.03622) | [code](https://github.com/microsoft/visualization-of-thought) | 
| I Know About "Up"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction | Meng *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2407.14133) | [code](https://github.com/zhouhao028/Iknow_up) | 
| Do Large Language Models have Spatial Cognitive Abilities? | Wu *et al.* | ACM Transactions on Intelligent Systems and Technology | [paper](https://dl.acm.org/doi/abs/10.1145/3716855) | [code](https://github.com/LLING000/SCABenchmark) | 
| Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models | Zhou *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.13872) | / | 
| SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding | Wu *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2505.17012) | [code](https://github.com/haoningwu3639/SpatialScore/) | 
| ***Post-training OR Fine-tuning*** |||||
| CityGPT: Empowering Urban Spatial Cognition of Large Language Models | Feng *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2406.13948) | [code](https://github.com/tsinghua-fib-lab/CityGPT) |
| Video-R1: Reinforcing Video Reasoning in MLLMs | Feng *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.21776) | [code](https://github.com/tulerfeng/Video-R1) |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | Liao *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.00883) | / |
| Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning | Ouyang *et al.* | Arxiv 2024 (Apr) | [pdf](https://arxiv.org/pdf/2504.01805) | [code](https://github.com/OuyangKun10/Spatial-R1) |
| MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse | Pan *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.18470) | [code](https://github.com/PzySeere/MetaSpatial) | 
| Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning | Tang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2410.16162) | / | 
| ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models | Ko *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.19355) | [code](https://github.com/mlvlab/ST-VLM) | 
| Imagine while Reasoning in Space: Multimodal Visualization-of-Thought |  Li *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.07542) | / | 
| SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models |  Ray *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2412.07755) | [code](https://arijitray.com/SAT/) | 
| Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models |  Xu *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.17015) | [code](https://github.com/facebookresearch/Multi-SpatialMLLM) | 
| SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models | Guo *et al.*   | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13112)  | [code](https://github.com/XiandaGuo/Drive-MLLM) |
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | Wu *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.12542) | / |
| ***Explainability*** |||||
| Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models | Rajabi *et al.* | ICLR 2024 Workshop | [paper](https://arxiv.org/pdf/2308.09778) | / |
| Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models | Qi *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.17349) | / |
| Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas | Chen *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.01773) | [code](https://github.com/shiqichen17/AdaptVis) | 
| ***Architecture*** |||||
| MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving | Zhang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2504.00379) | / |
| Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs | Ranasinghe *et al.* | CVPR 2024 | [paper](https://arxiv.org/pdf/2404.07449) | / |
| Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence | Wu *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.23747) | [code](https://github.com/diankun-wu/Spatial-MLLM) |
| ***More Benchmark and Dataset*** |||||
| Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding | Kabir *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.12663) | [code](https://github.com/Imran2205/LogicRAG) |
| How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game | Wang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.10042) | [code](https://github.com/THUNLP-MT/EscapeCraft) |
| ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models | Roberts *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/abs/2502.09696) | [code](https://zerobench.github.io/) |
| LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations | Xu *et al.* | WACV 2025 | [paper](https://arxiv.org/abs/2412.06322) | [code](https://github.com/Endlinc/LLaVA-SpaceSGG) |
| LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding | Li *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.08282) | [code](https://github.com/appletea233/LLaVA-ST) |
| PUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns | Chia *et al.* | ACL 2024 | [paper](https://arxiv.org/abs/2403.13315) | [code](https://github.com/declare-lab/LLM-PuzzleTest) |
| EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models | Du *et al.* | ACL 2024 | [paper](https://arxiv.org/abs/2406.05756) | [code](https://github.com/mengfeidu/EmbSpatial-Bench) |
| AirVista: Empowering UAVs with 3D Spatial Reasoning Abilities Through a Multimodal Large Language Model Agent | Lin *et al.* | ITSC 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10919532) | / |
| VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena | Parcalabescu *et al.* | ACL 2022 | [paper](https://arxiv.org/abs/2112.07566) | [code](https://github.com/Heidelberg-NLP/VALSE) |
| Things not written in text: Exploring spatial commonsense from visual signals | Liu *et al.* | ACL 2022 | [paper](https://arxiv.org/abs/2203.08075) | [code](https://github.com/xxxiaol/spatial-commonsense) |
| 2.5D Visual Relationship Detection | Su *et al.* | arxiv 2021(Apr) | [paper](https://arxiv.org/abs/2104.12727) | [code](https://github.com/google-research-datasets/2.5vrd) |
| PIP: Physical Interaction Prediction via Mental Simulation with Span Selection | Duan *et al.* | arxiv 2021(Sep) | [paper](https://arxiv.org/abs/2109.04683) | / |
| TVQA+: Spatio-temporal grounding for video question answering | Lei *et al.* | ACL 2020 | [paper](https://arxiv.org/abs/1904.11574) | [code](https://github.com/jayleicn/TVQAplus) |
| Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D | Goyal *et al.* | NIPS 2020 | [paper](https://arxiv.org/abs/2012.01634) | [code](https://github.com/princeton-vl/Rel3D) |
| Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates | Collell *et al.* | AAAI 2018 | [paper](https://arxiv.org/abs/1711.06821) | [code](https://github.com/gcollell/spatial-commonsense) |
| Visual Genome: Connecting language and vision using crowdsourced dense image annotations | Krishna *et al.* | IJCV 2017 | [paper](https://arxiv.org/abs/1602.07332) | [code](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) |
| Stating the Obvious: Extracting Visual Common Sense Knowledge | Yatskar *et al.* | NAACL 2016 | [paper](https://aclanthology.org/N16-1023/) | / |