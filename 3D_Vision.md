
# Spatial Reasoning in 3D Vision

### 3D Visual Grounding
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance | Dong *et al.* | ICCV 2023 | [paper](https://arxiv.org/pdf/2303.16894) | [code](https://github.com/Ivan-Tang-3D/ViewRefer3D) |
| Visual programming for zero-shot open-vocabulary 3d visual grounding | Yuan *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) | [code](https://github.com/CurryYuan/ZSVG3D) |
| ðŸ”¥ LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent | Yang *et al.* | ICRA 2024 | [paper](https://arxiv.org/pdf/2309.12311) | [code](https://github.com/sled-group/chat-with-nerf) |
| Data-Efficient 3D Visual Grounding via Order-Aware Referring | Tung-Yu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.16539) | [code](https://github.com/tony10101105/Vigor) |
| Grounded 3D-LLM with Referent Tokens | Chen *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.10370) | [code](https://github.com/OpenRobotLab/Grounded_3D-LLM) |
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Yang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.01584) | [code](https://github.com/AnjieCheng/SpatialRGPT) |
| ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities | Zhu *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2407.01525) | [code](https://github.com/ZCMax/ScanReason) |
| Vlm-grounder: A vlm agent for zero-shot 3d visual grounding | Xu *et al.* | CoRL 2024 | [paper](https://arxiv.org/pdf/2410.13860) | [code](https://github.com/OpenRobotLab/VLM-Grounder) |
| SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding | Rong *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.04383) | [code](https://github.com/iris0329/SeeGround) |
| ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning | Zhenyang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2503.23297) | / |
| Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis | Huang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.22420) | / |
| 3DAxisPrompt: Promoting the 3D grounding and reasoning in GPT-4o | Liu *et al.* | Neurocomputing 2025 | [paper](https://www.sciencedirect.com/science/article/pii/S0925231225007441) | / |
| CityAnchor: City-scale 3D Visual Grounding with Multi-modality LLMs | Li *et al.* | ICLR 2025 | [paper](https://openreview.net/pdf?id=7nOl5W6xU4) | [code](https://github.com/WHU-USI3DV/CityAnchor) |
| Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection | Zhu *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.01401) | [Code](https://github.com/OpenRobotLab/EaSe) |
| Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions | Zhu *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2504.04744) | [Code](https://github.com/cn-hezhu/LMAffordance3D) |


### 3D Scene Reasoning and QA
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment | Zhu *et al.* | ICCV 2023 | [paper](https://arxiv.org/pdf/2308.04352) | [code](https://github.com/3d-vista/3D-VisTA) |
| Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes | WangÂ *et al.* | Arxiv 2023 (Aug) | [paper](https://arxiv.org/pdf/2308.08769) | [code](https://github.com/Chat-3D/Chat-3D) |
| Situational awareness matters in 3d vision language reasoning | Man *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf) | [code](https://github.com/YunzeMan/Situation3D) |
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) |
| Gpt4point: A unified framework for point-language understanding and generation | Qi *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf) | [code](https://github.com/Pointcept/GPT4Point) |
| An Embodied Generalist Agent in 3D World | Huang *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2311.12871) | [code](https://github.com/embodied-generalist/embodied-generalist) |
| Agent3D-Zero: An Agent for Zero-shot 3D Understanding | Zhang *et al.* | ECCV 2024 | [paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02877.pdf) | / |
| ShapeLLM: Universal 3D Object Understanding for Embodied Interaction | QiÂ *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2402.17766) | [code](https://github.com/qizekun/ShapeLLM) |
| MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors | TangÂ *et al.* | MM 2024 | [paper](https://arxiv.org/pdf/2405.01413) | [code](https://github.com/TangYuan96/MiniGPT-3D) |
| Think-Program-reCtify: 3D Situated Reasoning with Large Language Models | He *et al.* | Arxiv 2024 (Apr) | [paper](https://arxiv.org/pdf/2404.14705) | [code](https://github.com/QingrongH/LLM-TPC) |
| Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers | HuangÂ *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2312.08168) | [code](https://github.com/ZzZZCHS/Chat-Scene) |
| Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models | LiuÂ *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2402.03327) | / |
| SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors | MaÂ *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2403.13438) | [code](https://github.com/dannymcy/zeroshot_task_hallucination_code?tab=readme-ov-file) |
| 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understand | XiongÂ *et al.* | TMM 2025 | [paper](https://arxiv.org/pdf/2501.07819) | [code](https://github.com/hmxiong/3UR-LLM) |
| Scene-llm: Extending language model for 3d visual understanding and reasoning | FuÂ *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.11401) | / |
| 3dmit: 3d multi-modal instruction tuning for scene understanding | LiÂ *et al.* | ICMEW 2024 | [paper](https://arxiv.org/pdf/2401.03201) | [code](https://github.com/staymylove/3DMIT) |
| Llava-3d: A simple yet effective pathway to empowering lmms with 3d-awareness | ZhuÂ *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2409.18125) | [code](https://github.com/ZCMax/LLaVA-3D) |
| Robin3D : Improving 3D Large Language Model via Robust Instruction Tuning | KangÂ *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.00255) | [code](https://github.com/WeitaiKang/Robin3D) |
| 3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding | Zemskova *et al.* | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.18450) | [code](https://github.com/CognitiveAISystems/3DGraphLLM) |
| LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences | ZhiÂ *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.01292v1) | [code](https://github.com/Hoyyyaard/LSceneLLM) |
| 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer | DengÂ *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2501.01163) | / |
| GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models | Qi *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.01428) | [code](https://github.com/Qi-Zhangyang/gpt4scene) |
| SplatTalk: 3D VQA with Gaussian Splatting | ZhangÂ *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.06271) | / |
| Empowering Large Language Models with 3D Situation Awareness | Yuan *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.23024) | / |
| Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning | Yu *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2503.00513) | [code](https://github.com/hanxunyu/Inst3D-LMM) |


### 3D Generation
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ***3D Layout Generation*** |||||
| Layoutgpt: Compositional visual planning and generation with large language models | Feng *et al.* | NIPS 2023 | [paper](https://arxiv.org/pdf/2305.15393) | [code](https://github.com/weixi-feng/LayoutGPT) |
| Layout Generation Agents with Large Language Models | Sasazawa *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.08037) | [code](https://github.com/ckdjrkffz/layout-agent) |
| LLplace: The 3D Indoor Scene Layout Generation and Editing via Large Language Model | Yang *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2406.03866) | / |
| Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM | Liao *et al.* | Arxiv 2024 (Jul) | [paper](https://arxiv.org/pdf/2407.21333) | / |
| Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit regularization | Wei *et al.* | Arxiv 2024 (Aug) | [paper](https://arxiv.org/pdf/2403.12848v2) | [code](https://github.com/weiyao1996/Planner3D) |
| EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing | Zheng *et al.* | ICLR 2025 | [paper](https://arxiv.org/pdf/2410.12836) | [code](https://github.com/eric-ai-lab/EditRoom) |
| Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint | Zhou *et al.* | ICLR 2025 | [paper](https://arxiv.org/pdf/2410.15391) | [code](https://github.com/Colezwhy/Layout-Your-3D) |
| LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models | Sun *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.02193) | [code](https://github.com/sunfanyunn/LayoutVLM) |
| ***3D Scene Generation*** |||||
| Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback | Lin *et al.* | Arxiv 2023 (May) | [paper](https://arxiv.org/pdf/2305.15808) | / |
| Gala3d: Towards text-to-3dÂ complex scene generation viaÂ layout-guided generative gaussian splatting | Zhou *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2402.07207) | [code](https://github.com/VDIGPKU/GALA3D) |
| SceneTeller: Language-to-3D Scene Generation | Â Ã–cal *et al.* | ECCV 2024 | [paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11481.pdf) | [code](https://github.com/sceneteller/SceneTeller) |
| Compositional 3D-aware Video Generation with LLM Director | Zhu *et al.* | NIPS 2024 | [paper](https://papers.nips.cc/paper_files/paper/2024/file/edbeca7811f9365c924c72a8a9bce83a-Paper-Conference.pdf) | / |
| Hierarchically-Structured Open-Vocabulary Indoor Scene Synthesis with Pre-trained Large Language Model | Sun *et al.* | AAAI 2025 | [paper](https://arxiv.org/pdf/2502.10675) | / |
| ***3D Generation as Program*** |||||
| ðŸ”¥ 3d-gpt: Procedural 3d modeling with large language models | Sun *et al.* | Arxiv 2023 (Oct) | [paper](https://arxiv.org/pdf/2310.12945) | [code](https://github.com/Chuny1/3DGPT) |
| SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code | Hu *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2403.01248) | / |
| SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements | Tam *et al.* | 3DV 2025 | [paper](https://arxiv.org/pdf/2408.02211) | [code](https://github.com/3dlg-hcvc/smc) |
| CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs | Wang *et al.* | AAAI 2025 | [paper](https://arxiv.org/pdf/2412.19663) | [code](https://github.com/SiyuWang0906/CAD-GPT) |
| From 2d cad drawings to 3d parametric models: A vision-language approach | Wang *et al.* | AAAI 2025 | [paper](https://arxiv.org/pdf/2412.11892) | / |
| CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images | Chen *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2504.04753) | / |
| CAD-Recode: Reverse Engineering CAD Code from Point Clouds | Rukhovich *et al.* | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.14042) | [code](https://github.com/filaPro/cad-recode) |
| ShapeLib: Designing a library of procedural 3D shape abstractions with Large Language Models | JONES *et al.* | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.08884) | / |







___

| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ***Benchmark and Dataset*** |||||
| SQA3D: Situated Question Answering in 3D Scenes | Ma *et al.* | ICLR 2023 | [paper](https://arxiv.org/pdf/2210.07474) | [code](https://github.com/SilongYong/SQA3D) |
| 3D Concept Learning and Reasoning from Multi-View Images | Hong *et al.* | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf) | [code](https://github.com/evelinehong/3D-CLR-Official) |
| M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts | Li *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2312.10763) | [code](https://github.com/OpenM3D/M3DBench/) |
| ðŸ”¥ MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations | Wang *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/5aed0d900297bd5593afc14ff452d4a8-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/OpenRobotLab/EmbodiedScan) |
| Multi-modal situated reasoning in 3d scenes | Linghu *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/feaeec8ec2d3cb131fe18517ff14ec1f-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/MSR3D/MSR3D) |
| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models | Lyu *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.14852) | [code](https://github.com/jiayuww/SpatialEval) |
| 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination | Yang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2406.05132) | [code](https://github.com/sled-group/3D-GRAND) |
| SPARTUN3D: SITUATED SPATIAL UNDERSTANDING OF 3D WORLD IN LARGE LANGUAGE MODELS | Yue *et al.* | ICLR 2025 | [paper](https://arxiv.org/pdf/2410.03878) | / |
| Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space | Zhang *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.11094) | [code](https://github.com/WeichenZh/Open3DVQA) |
| The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models? | Deng *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.04540) | [code](https://3d-llm.xyz/) |

| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         | Training-needed       | Training-free       |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|------------------|------------------|
| ***3D Scene Understanding and Layout Reasoning*** |||||
| An Embodied Generalist Agent in 3D World | Jiangyong *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2311.12871) | [code](https://github.com/embodied-generalist/embodied-generalist) | âˆš | |
| 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment | Ziyu *et al.* | ICCV 2023 | [paper](https://arxiv.org/pdf/2308.04352) | [code](https://github.com/3d-vista/3D-VisTA) | âˆš | |
| Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers | Haifeng *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2312.08168) | [code](https://github.com/ZzZZCHS/Chat-Scene) | âˆš | |
| Situational awareness matters in 3d vision language reasoning | Man *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf) | [code](https://github.com/YunzeMan/Situation3D) | âˆš | |
| 3dmit: 3d multi-modal instruction tuning for scene understanding | Li *et al.* | ICMEW 2024 | [paper](https://arxiv.org/pdf/2401.03201) | [code](https://github.com/staymylove/3DMIT) | âˆš | |
| Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models | Dingning *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2402.03327) | / | âˆš | |
| Scene-llm: Extending language model for 3d visual understanding and reasoning | Fu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.11401) | / | âˆš | |
| Agent3D-Zero: An Agent for Zero-shot 3D Understanding | Sha *et al.* | ECCV 2024 | [paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02877.pdf) | / | | âˆš |
| SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors | Chenyang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2403.13438) | [code](https://github.com/dannymcy/zeroshot_task_hallucination_code?tab=readme-ov-file) | | âˆš |
| Think-Program-reCtify: 3D Situated Reasoning with Large Language Models | He *et al.* | Arxiv 2024 (Apr) | [paper](https://arxiv.org/pdf/2404.14705) | [code](https://github.com/QingrongH/LLM-TPC) | | âˆš |
| Robin3D : Improving 3D Large Language Model via Robust Instruction Tuning | Weitai *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.00255) | [code](https://github.com/WeitaiKang/Robin3D) | âˆš | |
| LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences | Hongyan *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.01292v1) | [code](https://github.com/Hoyyyaard/LSceneLLM) | âˆš | |
| 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understand | Haomiao *et al.* | TMM 2025 | [paper](https://arxiv.org/pdf/2501.07819) | [code](https://github.com/hmxiong/3UR-LLM) | âˆš | |
| SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models |  Zantout *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2504.18684v1) | [code](https://github.com/nzantout/SORT3D) | | âˆš |
| ***VQA*** |||||
| Context-aware Alignment and Mutual Masking for 3D-Language Pre-training | Zhao *et al.* | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf) | [code](https://github.com/leolyj/3D-VLP) | âˆš | |
| Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes | Zehan *et al.* | Arxiv 2023 (Aug) | [paper](https://arxiv.org/pdf/2308.08769) | [code](https://github.com/Chat-3D/Chat-3D) | âˆš | |
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) | âˆš | |
| Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs | Ranasinghe *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ranasinghe_Learning_to_Localize_Objects_Improves_Spatial_Reasoning_in_Visual-LLMs_CVPR_2024_paper.pdf) | / | âˆš | |
| Gpt4point: A unified framework for point-language understanding and generation | Qi *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf) | [code](https://github.com/Pointcept/GPT4Point) | âˆš | |
| 3dmit: 3d multi-modal instruction tuning for scene understanding | LiÂ *et al.* | ICMEW 2024 | [paper](https://arxiv.org/pdf/2401.03201) | [code](https://github.com/staymylove/3DMIT) | âˆš | |
| Unifying 3D Vision-Language Understanding via Promptable Queries | Ziyu *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2405.11442) | [code](https://github.com/PQ3D/PQ3D) | âˆš | |
| ShapeLLM: Universal 3D Object Understanding for Embodied Interaction | Zekun *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2402.17766) | [code](https://github.com/qizekun/ShapeLLM) | âˆš | |
| MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors | Yuan *et al.* | MM 2024 | [paper](https://arxiv.org/pdf/2405.01413) | [code](https://github.com/TangYuan96/MiniGPT-3D) | âˆš | |
| Llava-3d: A simple yet effective pathway to empowering lmms with 3d-awareness | Zhu *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2409.18125) | [code](https://github.com/ZCMax/LLaVA-3D) | âˆš | |
| 3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding | Zemskova *et al.* | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.18450) | [code](https://github.com/CognitiveAISystems/3DGraphLLM) | âˆš | |
| 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer | Jiajun *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.01163) | / | âˆš | |
| SplatTalk: 3D VQA with Gaussian Splatting | Thai *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.06271) | / | âˆš | |
| Empowering Large Language Models with 3D Situation Awareness | Yuan *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.23024) | / | âˆš | |
| Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning | Yu *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2503.00513) | [code](https://github.com/hanxunyu/Inst3D-LMM) | âˆš | |
| ***3D Compositional Generation*** |||||
| ðŸ”¥ CityDreamer: Compositional Generative Model of Unbounded 3D Cities | Haozhe *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_CVPR_2024_paper.pdf) | [code](https://github.com/hzxie/CityDreamer) | LLM |
