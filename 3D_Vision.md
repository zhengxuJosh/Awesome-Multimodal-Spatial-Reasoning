
# Spatial Reasoning in 3D Vision

| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ***Benchmark and Dataset*** |||||
| SQA3D: Situated Question Answering in 3D Scenes | Xiaojian *et al.* | ICLR 2023 | [paper](https://arxiv.org/pdf/2210.07474) | [code](https://github.com/SilongYong/SQA3D) |
| M3DBench: Letâ€™s Instruct Large Models with Multi-modal 3D Prompts | Mingsheng *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2312.10763) | [code](https://github.com/OpenM3D/M3DBench/) |
| ðŸ”¥ MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations | Lyu *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/5aed0d900297bd5593afc14ff452d4a8-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/OpenRobotLab/EmbodiedScan) |
| ***3D Scene Understanding and Layout Reasoning*** |||||
| An Embodied Generalist Agent in 3D World | Jiangyong *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2311.12871) | [code](https://github.com/embodied-generalist/embodied-generalist) |
| 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment | Ziyu *et al.* | ICCV 2023 | [paper](https://arxiv.org/pdf/2308.04352) | [code](https://github.com/3d-vista/3D-VisTA) |
| Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers | Haifeng *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2312.08168) | [code](https://github.com/ZzZZCHS/Chat-Scene) |
| Situational awareness matters in 3d vision language reasoning | Man *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf) | [code](https://github.com/YunzeMan/Situation3D) |
| Multi-modal situated reasoning in 3d scenes | Linghu *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/feaeec8ec2d3cb131fe18517ff14ec1f-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/MSR3D/MSR3D) |
| Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models | Dingning *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2402.03327) | / |
| Scene-llm: Extending language model for 3d visual understanding and reasoning | Fu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.11401) | / |
| Agent3D-Zero: An Agent for Zero-shot 3D Understanding | Sha *et al.* | ECCV 2024 | [paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02877.pdf) | / |
| SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors | Chenyang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2403.13438) | [code](https://github.com/dannymcy/zeroshot_task_hallucination_code?tab=readme-ov-file) |
| Robin3D : Improving 3D Large Language Model via Robust Instruction Tuning | Weitai *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.00255) | [code](https://github.com/WeitaiKang/Robin3D) |
| SPARTUN3D: SITUATED SPATIAL UNDERSTANDING OF 3D WORLD IN LARGE LANGUAGE MODELS | Yue *et al.* | ICLR 2025 | [paper](https://arxiv.org/pdf/2410.03878) | / |
| LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences | Hongyan *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.01292v1) | [code](https://github.com/Hoyyyaard/LSceneLLM) |
| 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understand | Haomiao *et al.* | TMM 2025 | [paper](https://arxiv.org/pdf/2501.07819) | [code](https://github.com/hmxiong/3UR-LLM) |
| ***SLAM based on VLM*** |||||
| Lp-slam: language-perceptive RGB-D SLAM framework exploiting large language model | Zhang *et al.* | Arxiv 2023 (Mar) | [paper](https://arxiv.org/abs/2303.10089) | / |
| Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding | Kassab *et al.* | ICRA 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10610341) | / |
| Learning from Feedback: Semantic Enhancement for Object SLAM Using Foundation Models | Hong *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.06752) | / |
| ***VQA*** |||||
| Context-aware Alignment and Mutual Masking for 3D-Language Pre-training | Zhao *et al.* | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf) | [code](https://github.com/leolyj/3D-VLP) |
| 3D Concept Learning and Reasoning from Multi-View Images | Yining *et al.* | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf) | [code](https://github.com/evelinehong/3D-CLR-Official) |
| Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes | Delitzas *et al.* | BMVC 2023(oral) | [paper](https://arxiv.org/pdf/2306.02329) | [code](https://github.com/AlexDelitzas/Multi-CLIP) |
| Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes | Zehan *et al.* | Arxiv 2023 (Aug) | [paper](https://arxiv.org/pdf/2308.08769) | [code](https://github.com/Chat-3D/Chat-3D) |
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) |
| Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs | Ranasinghe *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ranasinghe_Learning_to_Localize_Objects_Improves_Spatial_Reasoning_in_Visual-LLMs_CVPR_2024_paper.pdf) | / |
| Unifying 3D Vision-Language Understanding via Promptable Queries | Ziyu *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2405.11442) | [code](https://github.com/PQ3D/PQ3D) |
| ShapeLLM: Universal 3D Object Understanding for Embodied Interaction | Zekun *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2402.17766) | [code](https://github.com/qizekun/ShapeLLM) |
| MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors | Yuan *et al.* | MM 2024 | [paper](https://arxiv.org/pdf/2405.01413) | [code](https://github.com/TangYuan96/MiniGPT-3D) |
| Llava-3d: A simple yet effective pathway to empowering lmms with 3d-awareness | Zhu *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2409.18125) | [code](https://github.com/ZCMax/LLaVA-3D) |
| 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer | Jiajun *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.01163) | / |
| SplatTalk: 3D VQA with Gaussian Splatting | Thai *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.11094) | / |
| Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space | Weichen *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.11094) | [code](https://github.com/WeichenZh/Open3DVQA) |
| ***3D Grounding*** |||||
| InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring | Zhihao *et al.* | ICCV 2021 | [paper](https://arxiv.org/pdf/2103.01128) | [code](https://github.com/CurryYuan/InstanceRefer) |
| LanguageRefer: Spatial-Language Model for 3D Visual Grounding | Roh *et al.* | CoRL 2021 | [paper](https://arxiv.org/pdf/2107.03438) | [code](https://github.com/rohjunha/language-refer) |
| 3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds | Lichen *et al.* | ICCV 2021 | [paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf) | [code](https://github.com/zlccccc/3DVG-Transformer) |
| Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds | Jain *et al.* | ECCV 2022 | [paper](https://arxiv.org/pdf/2112.08879) | [code](https://github.com/nickgkan/butd_detr) |
| EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding | Yanmin *et al.* | CVPR 2023 | [paper](https://arxiv.org/pdf/2209.14941) | [code](https://github.com/yanmin-wu/EDA) |
| Language Conditioned Spatial Relation Reasoning for 3D Object Grounding | Shizhe *et al.* | NIPS 2022 | [paper](https://arxiv.org/pdf/2211.09646) | / |
| Multi-View Transformer for 3D Visual Grounding | Shijia *et al.* | CVPR 2022 | [paper](https://arxiv.org/pdf/2204.02174) | [code](https://github.com/sega-hsj/MVT-3DVG) |
| UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding |  Zhenyu *et al.* | ICCV 2023 | [paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.pdf) | / |
| ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance | Dong *et al.* | ICCV 2023 | [paper](https://arxiv.org/pdf/2303.16894) | [code](https://github.com/Ivan-Tang-3D/ViewRefer3D) |
| Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding | Unal *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2309.04561) | [code](https://github.com/ouenal/concretenet) |
| Visual programming for zero-shot open-vocabulary 3d visual grounding | Yuan *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) | [code](https://github.com/CurryYuan/ZSVG3D) |
| Multi-branch Collaborative Learning Network for 3D Visual Grounding | Zhipeng *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2407.05363) | [code](https://github.com/qzp2018/MCLN) |
| ðŸ”¥ LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent | Yang *et al.* | ICRA 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10610443) | [code](https://github.com/sled-group/chat-with-nerf) |
| Visual programming for zero-shot open-vocabulary 3d visual grounding | Yuan *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) | [code](https://github.com/CurryYuan/ZSVG3D) |
| G3-LQ: Marrying Hyperbolic Alignment with Explicit Semantic-Geometric Modeling for 3D Visual Grounding | Yuan *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_G3-LQ_Marrying_Hyperbolic_Alignment_with_Explicit_Semantic-Geometric_Modeling_for_3D_CVPR_2024_paper.pdf) | / |
| Viewpoint-Aware Visual Grounding in 3D Scenes | Xiangxi *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_Viewpoint-Aware_Visual_Grounding_in_3D_Scenes_CVPR_2024_paper.pdf) | / |
| MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding | Chun-Peng *et al.* | CVPR 2024 | [paper](https://arxiv.org/pdf/2403.03077) | [code](https://github.com/dfki-av/MiKASA-3DVG) |
| Data-Efficient 3D Visual Grounding via Order-Aware Referring | Tung-Yu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.16539) | [code](https://github.com/tony10101105/Vigor) |
| Grounded 3D-LLM with Referent Tokens | Chen *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.10370) | [code](https://github.com/OpenRobotLab/Grounded_3D-LLM) |
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Yang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.01584) | [code](https://github.com/AnjieCheng/SpatialRGPT) |
| SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding | Rong *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.04383) | [code](https://github.com/iris0329/SeeGround) |
| ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding | Qihang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2502.19247) | [code](https://github.com/pqh22/ProxyTransformation) |
| ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning | Zhenyang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2503.23297) | / |
| ***3D modeling*** |||||
| ConceptFusion: Open-set Multimodal 3D Mapping | Jatavallabhula *et al.* | RSS 2023 | [paper](https://arxiv.org/pdf/2302.07241) | [code](https://github.com/concept-fusion/concept-fusion) |
| ðŸ”¥LERF: Language Embedded Radiance Fields | Kerr *et al.* | ICCV 2023 (Oral) | [paper](https://arxiv.org/pdf/2303.09553) | [code](https://github.com/kerrj/lerf) |
| Weakly Supervised 3D Open-vocabulary Segmentation | Kunhao *et al.* | NIPS 2023 | [paper](https://arxiv.org/pdf/2305.14093) | [code](https://github.com/Kunhao-Liu/3D-OVS) |
| OpenMask3D: Open-Vocabulary 3D Instance Segmentation | Takmaz *et al.* | NIPS 2023 | [paper](https://arxiv.org/pdf/2306.13631) | [code](https://github.com/OpenMask3D/openmask3d) |
| Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding | Jin-Chuan *et al.* | CVPR 2024 | [paper](https://arxiv.org/pdf/2311.18482) | [code](https://github.com/buaavrcg/LEGaussians) |
| ðŸ”¥LangSplat: 3D Language Gaussian Splatting | Minghan *et al.* | CVPR 2024 (Highlight) | [paper](https://arxiv.org/pdf/2312.16084) | [code](https://github.com/minghanqin/LangSplat) |
| ðŸ”¥Gaussian Grouping: Segment and Edit Anything in 3D Scenes | Mingqiao *et al.* | ECCV 2024 | [paper](https://arxiv.org/pdf/2312.00732) | [code](https://github.com/lkeab/gaussian-grouping) |
| GARField: Group Anything with Radiance Fields | Chung Min *et al.* | CVPR 2024 | [paper](https://arxiv.org/pdf/2401.09419) | [code](https://github.com/chungmin99/garfield) |
| FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping | Yuzhou *et al.* | AAAI 2025 | [paper](https://arxiv.org/pdf/2406.01916) | [code](https://github.com/George-Attano/FastLGS-Ex) |
| OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding | Yanmin *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.02058) | [code](https://github.com/yanmin-wu/OpenGaussian) |
| LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding | Hao *et al.* | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.17635) | [code](https://github.com/lifuguan/LangSurf) |
| SLGaussian: Fast Language Gaussian Splatting in Sparse Views | Kangjie *et al.* | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.08331) | / |
| Open-NeRF: Towards Open Vocabulary NeRF Decomposition | Hao *et al.* | WACV 2024 | [paper](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.pdf) | / |
| SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints | Weiwen *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.15712) | / |
| ***3D Compositional Generation*** |||||
| ðŸ”¥ CityDreamer: Compositional Generative Model of Unbounded 3D Cities | Haozhe *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_CVPR_2024_paper.pdf) | [code](https://github.com/hzxie/CityDreamer) |
