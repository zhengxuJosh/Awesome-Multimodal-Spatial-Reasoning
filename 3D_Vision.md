
# Spatial Reasoning in 3D Vision

| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|------------------|----------------------------------------------|----------------------------------------------|
| ***Benchmark and Dataset*** |||||
| üî• MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations | Lyu *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/5aed0d900297bd5593afc14ff452d4a8-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/OpenRobotLab/EmbodiedScan) |
| ***3D Spatial Relationship Reasoning among objects*** |||||
| ***3D Scene Understanding and Layout Reasoning*** |||||
| Multi-modal situated reasoning in 3d scenes | Linghu *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/feaeec8ec2d3cb131fe18517ff14ec1f-Paper-Datasets_and_Benchmarks_Track.pdf) | [code](https://github.com/MSR3D/MSR3D) |
| LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences | Hongyan *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2412.01292v1) | [code](https://github.com/Hoyyyaard/LSceneLLM) |
| Scene-llm: Extending language model for 3d visual understanding and reasoning | Fu *et al.* | WACV 2025 | [paper](https://arxiv.org/pdf/2403.11401) | / |
| ***3D Navigation and Planning*** |||||
| üî• 3d-llm: Injecting the 3d world into large language models | Hong *et al.* | NIPS 2023 | [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/413885e70482b95dcbeeddc1daf39177-Paper-Conference.pdf) | [code](https://github.com/UMass-Embodied-AGI/3D-LLM) |
| Towards learning a generalist model for embodied navigation | Zheng *et al.* | CVPR2 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.pdf) | [code](https://github.com/zd11024/NaviLLM) |
| üî• OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning | Wang *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.01533) | [code](https://github.com/NVlabs/OmniDrive) |
| üî• Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning | Gu *et al.* | ICRA 2024 | [paper](https://ieeexplore.ieee.org/iel8/10609961/10609862/10610243.pdf?casa_token=M6hQGyV3IEUAAAAA:H00JN_qifw9A_KA4byE0qpcj0ITOkPIBw8mFd4iUj59djitLQpv7Sl6ng5u9LMQGFPahuAXvHilEWw) | [code](https://github.com/concept-graphs/concept-graphs) |
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) |
| Sg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation | Yin *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/098491b37deebbe6c007e69815729e09-Paper-Conference.pdf) | [code](https://github.com/bagh2178/SG-Nav) |
| ChatNav: Leveraging LLM to Zero-Shot Semantic Reasoning in Object Navigation | Zhu *et al.* | TCSVT 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10734363) | / |
| Navigation with VLM framework: Go to Any Language | Yin *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.02787) | / |
| TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation | Zhong *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.16425) | / |
| NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning | Lin *et al.* | TPAMI 2025 | [paper](https://ieeexplore.ieee.org/abstract/document/10938647) | [code](https://github.com/expectorlin/NavCoT) |
| Dynamic Path Navigation for Motion Agents with LLM Reasoning | Zhao *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.07323) | / |
| ***3D VLA*** |||||
| üî• 3d-vla: A 3d vision-language-action generative world model | Zhen *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2403.09631) | [Code](https://github.com/UMass-Embodied-AGI/3D-VLA) |
| ***SLAM based on VLM*** |||||
| ***VQA*** |||||
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) |
| Situational awareness matters in 3d vision language reasoning | Man *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf) | [code](https://github.com/YunzeMan/Situation3D) |
| Llava-3d: A simple yet effective pathway to empowering lmms with 3d-awareness | Zhu *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2409.18125) | [code](https://github.com/ZCMax/LLaVA-3D) |
| ***3D Grounding*** |||||
| üî• LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent | Yang *et al.* | ICRA 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10610443) | [code](https://github.com/sled-group/chat-with-nerf) |
| Visual programming for zero-shot open-vocabulary 3d visual grounding | Yuan *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) | [code](https://github.com/CurryYuan/ZSVG3D) |
| Grounded 3D-LLM with Referent Tokens | Chen *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.10370) | [code](https://github.com/OpenRobotLab/Grounded_3D-LLM) |
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Yang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.01584) | [code](https://github.com/AnjieCheng/SpatialRGPT) |
| ***3D modeling*** |||||
| ***3D Compositional Generation*** |||||

## 3D
- **3D Spatial Relationship Reasoning among objects**
  - e.g., ËãπÊûúÂú®È¶ôËïâÁöÑÂ∑¶Ëæπ/‰∏äÈù¢/‰∏ãÈù¢
- **3D Scene Understanding and Layout Reasoning**
  - Âú∫ÊôØ‰∏≠ÁöÑ3DÁêÜËß£
- **3D Navigation and Planning**
  - 3D LLM Á≥ªÂàó, Á©∫Èó¥ÂØºËà™

- **SLAM based on VLM**
  - Semantic SLAM
  - Traditional SLAM?

- **VOA**
  - SpatialVLM
  - SpatialGPT

- **3D Grounding**
  - e.g., *Reasoning 3D: Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models*

- **3D modeling**
  - Blender code

- **3D Compositional Generation**
  - e.g., VLMÈ¢ÑÊµãlayout

- **3D VLA**
  - e.g., 3D-VLA
