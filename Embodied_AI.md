# Spatial Reasoning in Embodied AI

| Title                                                                                             | Authors       | Venue/Date           | Paper Link                                   | Code                                         |
---------------------------------------------------------------------------------------------------|---------------|----------------------|----------------------------------------------|----------------------------------------------|
| ***Spatial Reasoning in Embodied AI*** |||||
| SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning | Liu *et al.* | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.10074) | / |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf) | [code](https://github.com/remyxai/VQASynth) |
| SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models | Yang *et al.* | NIPS 2024 | [paper](https://arxiv.org/pdf/2406.01584) | [code](https://github.com/AnjieCheng/SpatialRGPT) |
| SpatialBot: Precise Spatial Understanding with Vision Language Models | Cai *et al.* | Arxiv 2024 (Jun) | [paper](https://arxiv.org/pdf/2406.13642) | [code](https://github.com/BAAI-DCAI/SpatialBot) |
| RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics | Song *et al.* | ICLR 2025 (Workshop) | [paper](https://arxiv.org/pdf/2411.16537) | / |
| RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics |  Yuan *et al.* | CoRL 2024 | [paper](https://arxiv.org/pdf/2406.10721) | [code](https://github.com/wentaoyuan/RoboPoint) |
| Spatially Visual Perception for End-to-End Robotic Learning | Davies *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.17458) | / |
| 3D-Mem: 3DScene Memory for Embodied Exploration and Reasoning | Yang *et al.* | CVPR 2025 | [paper](https://arxiv.org/pdf/2411.17735) | [code](https://github.com/UMass-Embodied-AGI/3D-Mem) |
| Visual Agentic AI for Spatial Reasoning with a Dynamic API |  Marsili *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2502.06787) | [code](https://github.com/damianomarsili/VADAR) |
| An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models |  Yuan *et al.* | EMNLP 2024 | [paper](https://arxiv.org/pdf/2411.06048) | [code](https://github.com/FatemehShiri/Spatial-MM) |
| An Embodied Generalist Agent in 3D World |  Huang *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2311.12871v3) | [code](https://github.com/embodied-generalist/embodied-generalist) |
| What‚Äôs ‚Äúup‚Äù with vision-language models? Investigating their struggle with spatial reasoning |  Kamath *et al.* | EMNLP 2023 | [paper](https://arxiv.org/pdf/2310.19785) | [code](https://github.com/amitakamath/whatsup_vlms) |
| Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning |  Zhao *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.12680) | [code](https://github.com/EmbodiedCity/Embodied-R.code) |
| The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models? |  Zhang *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/abs/2504.04540) | / |
| Why Is Spatial Reasoning Hard for VLMs?An Attention Mechanism Perspective on Focus Areas |  Chen *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.01773) | [code](https://github.com/shiqichen17/AdaptVis) |
| ***Vision-language navigation (VLN)*** |||||
| üî• 3d-llm: Injecting the 3d world into large language models | Hong *et al.* | NIPS 2023 | [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/413885e70482b95dcbeeddc1daf39177-Paper-Conference.pdf) | [code](https://github.com/UMass-Embodied-AGI/3D-LLM) |
| Towards learning a generalist model for embodied navigation | Zheng *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.pdf) | [code](https://github.com/zd11024/NaviLLM) |
| üî• OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning | Wang *et al.* | Arxiv 2024 (May) | [paper](https://arxiv.org/pdf/2405.01533) | [code](https://github.com/NVlabs/OmniDrive) |
| üî• Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning | Gu *et al.* | ICRA 2024 | [paper](https://ieeexplore.ieee.org/iel8/10609961/10609862/10610243.pdf?casa_token=M6hQGyV3IEUAAAAA:H00JN_qifw9A_KA4byE0qpcj0ITOkPIBw8mFd4iUj59djitLQpv7Sl6ng5u9LMQGFPahuAXvHilEWw) | [code](https://github.com/concept-graphs/concept-graphs) |
| Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning | Chen *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf) | [code](https://github.com/Open3DA/LL3DA) |
| Sg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation | Yin *et al.* | NIPS 2024 | [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/098491b37deebbe6c007e69815729e09-Paper-Conference.pdf) | [code](https://github.com/bagh2178/SG-Nav) |
| ChatNav: Leveraging LLM to Zero-Shot Semantic Reasoning in Object Navigation | Zhu *et al.* | TCSVT 2024 | [paper](https://ieeexplore.ieee.org/abstract/document/10734363) | / |
| Navigation with VLM framework: Go to Any Language | Yin *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.02787) | / |
| TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation | Zhong *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.16425) | / |
| NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning | Lin *et al.* | TPAMI 2025 | [paper](https://ieeexplore.ieee.org/abstract/document/10938647) | [code](https://github.com/expectorlin/NavCoT) |
| Dynamic Path Navigation for Motion Agents with LLM Reasoning | Zhao *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.07323) | / |
| AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning | Kong *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.07557) | / |
| Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation | Ling *et al.* | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.08806) | / |
|  |  |  |  |  | 
| ***Embodied Question Answering*** |||||
| EMBOSR: Embodied Spatial Reasoning for Enhanced Situated Question Answering in 3D Scenes | Yu *et al.* | IROS 2024 | [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10801720) | / | 
| Knowledge-based embodied question answering | Tan *et al.* | TPAMI 2023 | [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10128752) | / | 
| OpenEQA: Embodied Question Answering in the Era of Foundation Models | Majumdar *et al.* | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/facebookresearch/open-eqa) | 
| ***Embodied Grasping*** |||||
| Free-form language-based robotic reasoning and grasping | Jiao *et al.* | Arxiv 2025(Mar) | [paper](https://arxiv.org/pdf/2503.13082) | / | 
| ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter | Qian *et al.* | Arxiv 2024(July) | [paper](https://arxiv.org/pdf/2503.13082) | [Code](https://github.com/H-Freax/ThinkGrasp) | 
| ***Vision-language-action models (VLAs)*** |||||
| ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation | Huang *et al.* | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2409.01652) | [Code](https://github.com/huangwl18/ReKep) |
| PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs | Nasiriany *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2402.07872v1) | / |
| VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models | Huang *et al.* | CoRL 2023 | [paper](https://arxiv.org/pdf/2307.05973v2) | [Code](https://github.com/huangwl18/VoxPoser) |
| An Embodied Generalist Agent in 3D World | Huang *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2311.12871) | [Code](https://github.com/embodied-generalist/embodied-generalist) |
| 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations |  Ke *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2402.10885) | [Code](https://github.com/nickgkan/3d_diffuser_actor) |
| RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation | Vecerik *et al.* | ICRA 2024 | [paper](https://arxiv.org/pdf/2308.15975) | / |
||||||
| RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | Brohan *et al.* | CoRL 2023 | [paper](https://arxiv.org/pdf/2307.15818) | / |
| OpenVLA: An Open-Source Vision-Language-Action Model |  Kim *et al.* | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2406.09246) | [Code](https://github.com/openvla/openvla) |
| üî• 3d-vla: A 3d vision-language-action generative world model | Zhen *et al.* | ICML 2024 | [paper](https://arxiv.org/pdf/2403.09631) | [Code](https://github.com/UMass-Embodied-AGI/3D-VLA) |
| SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model | Qu *et al.* | RSS 2025 | [paper](https://arxiv.org/abs/2501.15830) | [Code](https://github.com/SpatialVLA/SpatialVLA) |
| Gemini Robotics: Bringing AI into the Physical World | Gemini Robotics Team *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.20020) | / |
| œÄ0: A Vision-Language-Action Flow Model for General Robot Control | Black *et al.* | Arxiv 2024 (Oct) | [paper](https://arxiv.org/abs/2410.24164v1) | [Code](https://github.com/Physical-Intelligence/openpi) |
| œÄ0.5: a Vision-Language-Action Model with Open-World Generalization | Physical Intelligence | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2504.16054) | / |
| PointVLA: Injecting the 3D World into Vision-Language-Action Models | Li *et al.* | Arxiv 2025 (Mar) | [paper](https://arxiv.org/abs/2503.07511) | / | 
| ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge | Zhou *et al.* | Arxiv 2025 (May) | [paper](https://arxiv.org/abs/2505.21906) | / |
|  |  |  |  |  |
| ***Embodied World Model*** |||||
| UniSim: Learning Interactive Real-World Simulators | Yang *et al.* | ICLR 2024 | [paper](https://arxiv.org/pdf/2310.06114) | / |
|  |  |  |  |  |
<!--| ***Embodied Control*** ||||| -->
<!--|  |  |  |  |  | -->
<!-- Embodied World Model -->
<!-- Embodied Control -->
<!--| ***Embodied 3D Scene Understanding*** ||||| -->
<!--| ***Embodied 3D Visual Grounding*** ||||| -->
<!-- |  |  |  |  |  | -->
