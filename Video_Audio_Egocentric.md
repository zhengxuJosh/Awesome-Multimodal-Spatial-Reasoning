# Spatial Reasoning in Audio / Video

## Video
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         | 
|---------------------------------------------------------------------------------------------------|----------------------------------------------|----------------------------------------------|------------------|------------------|
| VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs | Cheng *et al.* | 2024 Oct. | [link](https://arxiv.org/pdf/2406.07476) | [code](https://github.com/DAMO-NLP-SG/VideoLLaMA2) |
| VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs | Liao *et al.* | ACL 2024 Findings | [link](https://arxiv.org/pdf/2409.20365) | - |
| Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model | Liu *et al.* | 2024 Nov. | [link](https://arxiv.org/pdf/2408.00754) | - | 
| Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces | Yang *et al.* | 2024 Dec | [link](https://arxiv.org/pdf/2412.14171) | [code](https://github.com/vision-x-nyu/thinking-in-space) | 
| Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning | Ouyang *et al.* | 2025 Apr | [link](https://arxiv.org/pdf/2504.01805) | [code](https://github.com/OuyangKun10/Spatial-R1) | 
| Video-R1: Reinforcing Video Reasoning in MLLMs | Feng *et al.* | 2025 Mar | [link](https://arxiv.org/pdf/2503.21776) | [code](https://github.com/tulerfeng/Video-R1) | 
| AETHER: Geometric-Aware Unified World Modeling | Aether Team & Shanghai AI Lab| 2025 Mar | [link](https://arxiv.org/pdf/2503.18945) | [code](https://aether-world.github.io/) | 
| ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models | Ko *et al.* | 2025 Mar | [link](https://arxiv.org/pdf/2503.19355) | [code](https://ikodoh.github.io/ST-VLM) | 
| Towards Fine-Grained Video Question Answering | Dai *et al.* | 2025 Mar | [link](https://arxiv.org/pdf/2503.06820) | - | 
| VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning | Qi *et al.* | 2025 Apr | [link](https://arxiv.org/pdf/2504.07956) | [code](https://github.com/zhishuifeiqian/VCR-Bench) |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | Liao *et al.* | 2025 Apr | [link](https://arxiv.org/pdf/2504.00883) | [code](https://github.com/zhijie-group/R1-Zero-VSI) |
| V-star: Benchmarking video-llms on video spatio-temporal reasoning | Cheng *et al.* | 2025 May | [link](https://arxiv.org/pdf/2503.11495) | - |

## Egocentric Video
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         | 
|---------------------------------------------------------------------------------------------------|----------------------------------------------|----------------------------------------------|------------------|------------------|
| Advancing Egocentric Video Question Answering with Multimodal Large Language Models | Patel  *et al.* | 2025 Apr | [link](https://arxiv.org/pdf/2504.04550) | - | 
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | Wu *et al.* | 2025 Apr | [link](https://arxiv.org/pdf/2503.12542) | [code](https://github.com/WPR001/Ego-ST) | 
| AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding | Suglia *et al.* | 2024 June | [link](https://arxiv.org/pdf/2406.13807) | [code](https://github.com/alanaai/EVUD) | 

## Audio
| Title                                                                                             | Authors       | Venue/Date       | Paper Link                                   | Code                                         | 
|---------------------------------------------------------------------------------------------------|----------------------------------------------|----------------------------------------------|------------------|------------------|
| CLIP-Powered TASS: Target-Aware Single-Stream Network for Audio-Visual Question Answering | Jiang *et al.* | 2024 May | [link]([https://arxiv.org/pdf/2406.07476](https://arxiv.org/pdf/2405.07451)) | - |
| Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation | Du *et al.* | 2025 Mar. | [link](https://arxiv.org/pdf/2503.13068) | [code](https://github.com/GeWu-Lab/Crab) | 
| BAT: Learning to Reason about Spatial Sounds with Large Language Models | Zheng *et al.* | ICML 2024 | [link](https://arxiv.org/pdf/2402.01591) | [code](https://github.com/zszheng147/Spatial-AST) |
| Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamics Audio-Visual Scenarios | Jiang *et al.* | ACL 2023 Findings | [link]() | [code](https://arxiv.org/pdf/2305.12397) | - | 
| video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model | Sun *et al.* | 2025 Feb. | [link](https://arxiv.org/pdf/2502.11775) | [code](https://github.com/BriansIDP/video-SALMONN-o1) |  
